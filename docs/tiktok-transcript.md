# Transcript of Build TikTok's Personalized Real-Time Recommendation System in Python with Hopsworks

https://youtu.be/skZ1HcF7AsM?si=ZNnWA6hAHDTsELBb

https://arxiv.org/pdf/2209.07663

I gave a talk at PyData Berlin on how to build your own TikTok recommendation algorithm. The TikTok personalized recommendation engine is the world's most valuable AI. It's TikTok's differentiation. It updates recommendations within 1 second of you clicking - at human perceivable latency. If your AI recommender has poor feature freshness, it will be perceived as slow, not intelligent - no matter how good the recommendations are.
TikTok's recommender is partly built on European Technology (Apache Flink for real-time feature computation), along with Kafka, and distributed model training infrastructure. The Monolith paper is misleading that the 'online training' is key. It is not. It is that your clicks are made available as features for predicitons in less than 1 second. You need a per-event stream processing architecture for this (like Flink - Feldera would be my modern choice as an incremental streaming engine).

thanks thanks Anna I don't want to contradict you but you're fine to walk at I won't take it personally it's cool I know how it is right I spent the last few minutes reimplementing this uh this workshop for collab because I didn't realize the internet sucked um in the middle so people record okay yeah sure um so um so let's get let's jump in so what we're going to do I think I always like to start at the at the end and work our way backwards is we're going to do a demo of a Tik Tok recommender system and a bit I'm going to hack up a bit here so let me see while I'm here hopefully the internet's good uh Tik Tok stream L run okay let me just pull up uh a window here so what we're going to build is it's not a it's nothing too fancy right it's just um this one here um you basically everyone kind of knows how Tik Tok works you you C click on videos and and it it shows you some suggestions for videos you have like timelines that you follow and things like that and but what what it's well known for is having a really uh interactive recommender system so what that means is that um you click on a video that's sport or it's entertainment and within within a little bit of time it adapts its preferences to basically knowing that you want to look more at this particular topic than another one okay so um so what the what we're doing is we're going to write uh five programs and they are notebooks and I did have the link up let me show you the link up again you'll find all the links here on the slid so if you haven't taken down that URL please note it down now and um in there you'll find a link to the docs so the docs look like this if you find it build a Tik Tok recommender system I had designed it originally to work on Jupiter um but it ain't going to work look at the number in the room uh I measured the internet here it's a megabyte per second and we're going to install tensorflow which is about 600 megabytes so it ain't going to work uh is the way I look at it right so let's get started with what we're going to talk about and the internet seems to be very slow here so we'll let that work up in a sec um Tik Tok is really good right I have a I have two boys who are 12 and 14 the 14-y old has 30,000 followers um apparently he does really good slander he doesn't get it from me but anyway um the reason why it's really good is because it's it's it's it's personalized right it learns your preferences really really really quickly now many people interpret that to mean that it's training a new model within a few seconds right and in fact Tik Tok had a paper saying we train models really fast and that paper is called monolith and um they that's not the secret sauce right so the secret sauce for for Tik Tok is not that it updates the models every 5 minutes and learns exactly what you did 5 minutes ago go it learns it much faster than that it learns it within a few seconds and the way it does that is not by retraining the model but updating your features right what you've clicked on so if I click on a video that's sport or I click on a video That's Entertainment or slander or whatever that information needs to be available within a couple of seconds of making the next recommendation for the video I want to look at right and that's the infrastructure they built it on they built it on technology built here in Berlin Flink Apache Flink if you're curious we're not going to do Flink there is actually a repo with the Flink version of what we'll talk about here that scales to you know enormous volumes we're doing this in Python today um but basically that's the secret source that if you click on things and you make the information that you clicked on available very quickly we can use that information as input to our predictions so when you call model. predict that's what you do when you have a train model you call model. predict you can say well the last videos you looked at were sport and entertainment now the recommender can know okay uh maybe I should show more of that okay so um it's been called digital crack so Andre karthy from um openai called it digital crack and that kind of stuck as a name because it it it infects your brain it's AI that gets into you and one of the reasons why is this fast feedback loop that we can see at the bottom so when you want to look at the next video on Tik Tok uh what will happen is it'll go through hundreds of of millions of videos and get lots of what we call candidates that it's going to show next and the candidates will be in the order of hundreds and from those we want to personalize those candidates or rank them for you based on what you've done recently and then from there we'll get some recommended videos that go out there and the key points here I think that make Tik Tok different is you can see our total endtoend latency the time between you you clicking something and the recommender coming back will be very very low and we're not going to be that low today because we're doing it in p and so on and it's not engineered to be fast but um it will it that's the basic idea of uh Tik Tok and you can see we have this fast feedback loop what we tend to call that is we call them very fresh features that we want the features the input to the model to make predictions to not be stale or old they want to be a couple of seconds old at most if they're you know minutes old or days old it's it's not going to help it uh learn quickly right um I was going to show this one here let me see this is me going through a firewall let me not show the firewall because the firewall is going to suck I have this one up here um let me okay is this one loaded this is actually the video recommend I was going to show earlier I'm running the whole thing as I'll reload it here as a what's called a hooking face space has anyone heard of hooking face spaces you should oh Lots okay they're free it's really cool you get like 16 uh 16 meaby or gigabytes of of of uh memory and two cores and it doesn't cost you anything at the moment so what I what I did was I deployed a UI written in Python the framework called streamlet it's in the repo and the repo is linked in the code which is here this is the repo here and um what we end up at the very end is a web interface to make these predictions for us okay so you can pick your your you know your your user and then there's a buttons will come up here it's going a little bit slow internet but um that's basically what what we'll end up uh doing our predictions right this is the framework that I mentioned at Tik Tok it's called month and there's a research paper that they published about it and the paper tells you a lot about how they do um uh very frequent training of the models but the key thing here is this feedback loop when the user clicks on an action like you like them a video or if you uh you know dislike it I don't know if you can dislike it and but those basically get logged to an event bus called Kafka this is extremely scalable so Kafka can scale to scale to pedabytes and size and then they have petabyte siiz Flint clusters that process all your clicks and all your actions and they create features and those features are then used by our models so the features that we'll look at today are things like uh you know what type of category of of video did you click on how many likes are on the video how many people have viewed it uh what's the watch time of the video you can basically come up with many many more creative features but you don't need that many to be able to build a reasonable recommender uh the recommender Bas basically need to know in very short time what you just recently clicked on and did you spend a long time looking at it or a short time so you can create you can even use it you know yourself you can come up with features you can say well if I watch the video Until the End that's a good sign right if I just watched it for five seconds that's a sign that maybe I didn't like it you know you can impute or you can sorry you can infer uh that certain signals are that you liked it certain signals that you didn't you don't have to get people to click thumbs up or thumbs down to actually generate signals about whether um these worked or not so that's what we're going to look at today we're not going to look at the um the training uh we're going to look at training uh something called a two- Tower model we're not going to look at um this fast training in the monolith paper so um to start with uh let's just be clear that Tik Tok is not a a very good recommender from a social perspective um this is a you know a quote I read recently I'm from Ireland and this is a quote I read in the news last week and basically they what they did was they they took out a you know they pretended to be a 13-year-old who registered with Tik Tok and because it's able to learn your preferences very quickly and it doesn't have any guard rails um you know within 20 minutes you're looking at self harm and suicidal videos right that ain't good right so I'm not encouraging people to do that that's not the reason why we have the tutorial today the reason is to learn about the technology behind it and we can do better than this it's also in the news Tik Tok in the states are going to ban it allegedly um I don't maybe they did ban it did they at the weekend anyone know what did the vote go [Music] ahead sorry pass through the house not the Senate okay so it hasn't passed through the Senate so maybe it'll get through maybe it won't okay now this is another side I I I often show people why Tik Tok is interesting from a business perspective maybe you're thinking about doing a startup right you're saying well you know I'm kind of bored my job I want to do a startup you know where will the VCS be looking for money I'm we're from hopss hops is a VC back company so that's kind of where we go that I I worked a lot with the Flink guys when they're starting out so you know them really well um but what you do is you basically need to find an area where VCS are putting money and one area they're looking at money right now is realtime AI or realtime ML and the reason realtime ml is interesting is because you can generate a lot of business value Tik Tok is a very very profitable very very um rich company and you know that because the Americans are trying to shut it down um otherwise they wouldn't right so they're not trying to shut down hops Works yet uh which is my company but um so so realtime systems create a lot of value but they're not something you will learn typically in an ml course many of you have probably taken a data science course you've done a basic intro to machine learning you took a static data set which is at the bottom you split it up into training and test sets and you made some predictions on the test set and said you were done right um but that's not generating value a machine learning model that generates value takes new data in and makes predictions on it and the more predictions you make on more new data the more value you create very simple now you can do that with batch data you can make a prediction once a day which feels like that's what YouTube does like once a day maybe it updates the for you list um or you can do it the Tik Tock way so as you click and and click on things the system will be learning your preferences I use the term learning Loosely um the system will be updating your recent activities and recent features to give better predictions about what it should show to you so we're going to build um a an ml system I use the term ml system which isn't cool it should be AI system right because we we moved over from ml to AI I get confused it was it was ml it was AI 10 10 years ago 20 years ago then it became ML and now we're back to AI um but what one of the challenges in building an AI system is that we're not going to train we're not going to just train a model in your notebook and a static data set right that means we actually have to bring in new data and we have to train models on snapshots of that data because the data won't be static it's going to keep coming in or keep getting new data so we need to be able to take a static snapshot of that data it needs to be consistent and correct we're going to train our model and then we're going to get our model and new data that's coming in and make predictions on it okay are we going to do all of that in one big notebook no right that's not a good way to build an AI system so taking raw data training a model and making a prediction and one go is a crazy thing right crazy talk that's crazy talk um what we're going to do is is Mo do what we do in software engineering is we're going to build a modular system we're going to break it up into independent parts that can be composed together into a system all right so what we're going to do is we're going to build something called a feature pipeline a training pipeline an inference Pipeline and different examples of those there's about five or six programs here but they fall into these General categories so what a feature pipeline is sorry those at the back you might be able to see it I hope you have the slides but I'll say what a feature pipeline is it's basically a program in our case it's a Python program that takes as input the raw data and the output will be the features features and any labels we use to train our models and then the training pipeline will be the input will be the features and the labels and the output will be the train model and then the inference pipeline the input will be the model and the features and the output will be the predictions so this is a very natural uh breakdown of a a full AI system into three natural components one is we we do our data processing or feature engineering and other is we train our models and the third part is we make our predictions or inference so what we're going to do is you're going to write those programs but we're going to not going to start with Docker we're not going to start with kubernetes we're not going to build the ml infrastructure for this we're going to use a seress platform a company called hopworks that I work for and you're going to have to register an account so if you're going to want to follow the tutorial you can do this while I'm talking you'll have to go to app. hop. or just go to hop. and register an account and then we're going to use that as the platform that manages the data throughout this AI life cycle so when I have a feature program that creates a data frame I'm just going to write it to hopworks when I want to train a model I'm just going to read that data from hopworks train the model and save the model back to hopworks and when I want to make predictions I'm going to have a user interface and some input data will come in but I'm going to read more features and I'm going to download the model from hopss and then I'll make my predictions with that so that's how we're going to split up the uh or modularize our AI system and then hopss is the layer that helps us compose it together back into uh a system okay um it's you know that looks simple I think everyone looks kind of simple oh it's easy but you know you can make this obviously way more complex so our feature pipeline could be a batch or a streaming pipeline um I'm just going to show you a feature pipeline for creating some synthetic data to begin with we're going to run a notebook that will create synthetic data for us and uh it's going to write to an abstraction called a feature group a table containing our features and we're also going to create a vector index so this is an approxim nearest neighbor index and we're going to write that again into the a feature group in hopss it's an index for that table of features and then when we want to train our models we need to find the features that we want to use remember a feature is just the input to a model right so in our case it might be for a video it might be the number of likes on the video it might be the length of the video it might be you know the um the the category of the video but me as a user you know I may have some preferences as well I may have an age I may have a gender and I may have country I live in and so on we can use a lot of attributes of me or features of me to help make decisions about uh content to recommend so the basically we're going to work with pandas to create this features and we'll write them to hopworks when we want to train a model we're going to select features from these different feature groups and then we'll say this is the features I want for this model OKAY create some training data train the model save the model back to hopworks and then when we make predictions we're going to say hey give me some features because I have this user ID but I need to get features for them uh pre-computed features and then we'll use that to make predictions right I'm going to I'm going to start coding in a sec don't worry um I'm just going to introducing the abstractions this is the really the really most important one that when you're writing we're going to write to these things called feature groups they tables of think of them as pandas data frames that just keep growing so think of as being kind of mutable data for pandas data frames I write in a panda's data frame and another one and another one and the thing keeps growing and then when I want to get some data to train a model with I just say give me back Panda's data frames and I get it from it now you're not restricted to writing panels data frames you can have streaming pipelines or you can even have like tables from Snowflake that are mounted in there but when we want to read from the platform we're not going to read directly from the feature groups mostly we'll we'll use something called a feature view so we're going to say hey this model that I want to train it has some data in the user table some in the video's table and I want to join them together give me these features okay give me some training data and then later on we'll get some data back for predictions so there are the two abstractions the feature group we write to and the feature view we read from so let's get started on the feature pipeline I'm going to just um pull up because we're kind of trying to do an interactive demo I've been running this on my notebook um uh juper Notebook on my computer has anybody managed to do a pip install on this and I get it working no no one God all right okay so let's do collab instead I'm not a collab kind of guy but um because because it kind of sucks um here we go let me see so I so this notebook if you're curious we're going to go into this is the repo that we have here it's called Pi data- Tik Tock uh Jim Dowling is the the or that's my name and we're going to run this one called p uh feature backfill and what we're going to do is we're going to take videos users and interactions between them and I'll explain that in a little bit more detail but you can think about it I've got users in my video platform I've got the videos and in interaction is when a user clicks on a video or likes a video or does something with a with a video yes so there are the three tables that we'll have um I'll explain those in a little bit but I just want to get us started in coding so in Jupiter what I would normally do is just run the cells and we're we're good to go but I'm going to run it from collab instead so this is me earlier I just started it a few seconds ago and I wanted to download that let's have a look at this so one of the reasons why I didn't want to use collab all right mimesis I have to install here um one of the reasons I want to use collab as you can see I have to do something really ugly up here so when we talk about modularization and um python what happens if I want to pip install a library in a collab notebook let's open my notebook here and we'll have a look at it so you can see here I've got this uh module called features and I'm importing this users module sorry this package called features and this module called users so I remember I said I've got tables with users videos and interactions well I put my code in those modules I've got this directory here called features I we won't call users and if we look in here you've got a function basically to generate some users so since synthetic data it's going to give us back a um uh in this case you can see it's it's generating a a dict and um it's going to return a list of those users so it's going to return a list with this dict inside it um it's in a separate module but collab doesn't like that right so those of you who've used collab will know that won't work um there's no way to to kind of get these so what I did and I hacked out here um while I was waiting is I just downloading them here you can see that um so I actually forgot to download down one of them let's and then I wanted to pip install memesis so let's have a look at memesis here please shout out if you have any questions at this point there's one more Library I have to install here yeah you also announce for our online user that we have they can place their questions on slido and you will answer them okay so for all the online users uh you can ask questions on slido uh whatever that is I guess it's an online Tool uh okay so you can ask questions on slido and then then we'll bring them up here so I'm going to I I was missing this Library mimesis this will ask you to restart your shell which is fine um I did download that so this looks like it's installed okay so what I want to do here was um you can see import these modules so this is something that really sucks in collab what I don't like collab um that you want to organize your code into modules and the reason why you have a module in Python is because you want to reuse code right I have six different notebooks am I going to take the same function and redefine it six different times or four different times no it's insane that's non dry code I want dry code dry means do not repeat yourself um and uh so basically I put it in a module so what we're doing here is um generating some random users generating some random videos um Genera some some interactions between those users and videos so we've created our users we've created our videos this one will take a bit more time it's 250,000 um this one I won't spend too much time on but it's a very nice Library called Great Expectations in pandas or sorry in Python and what it does is you can Define data validation rules so you probably heard the term garbage in garbage out in data science you don't want any garbage in so what you can do with Great Expectations is you can Define data validation rules so for example here we've got a column called age and we're saying the minimum value for ages 12 and the maximum value is 100 this is very agist because there are people over 100 who use Tik Tok unsure um and there's definitely People Under 12 who use it excuse me and but I'm defining um on the pandas data frames that I'm going to write in here and basically I can Define any types of of data validation rules and we'll see that they get evaluated then when I want to write this data to our our data layer what we call a feature store hops works and we also got another one expect uh last time I looked we have three genders uh you know male female non-binary it's called other here um there may be more uh I don't know and um you can basically Define here we we've said there's only three if there's going to be four another one appears then it'll it'll give us a warning or an error um okay and this is again another data validation rule we don't need to go through them in too much detail so um another data validation rule so it's looking at watch time of videos and saying it should be U minimum a zero right so you shouldn't have any negative watch times this is where you'll probably get stuck at first which is it's trying to log in so what it's going to do it's going to ask you for API key in hopworks um am I going to show you my one uh probably I will have to we can delete it later um I don't see any way around this at the moment given the way it is um luckily it's not the end of the world um and now it's going to ask me for my project cuz I have a bunch of projects oh no it logged me in okay if you have more than one project in hopster It'll ask you for that but you won't have more than one I have more than one but anyway so what I can do is I can click on this link here hopefully it'll open up a new tab and um okay I need to log in oh I was logging in the wrong user sorry I've got too many accounts on hopss I'm logged in here so this is me logged in here and when you come into hopworks um what you'll see is that there's a bunch of things on the left hand ins side so this is where we're going to manage our data remember we said we're going to store our data in feature groups and we're going to have the feature views to have the the selections of features for training and then when we store a model it'll be down here in the model registry and then we'll have deployment so I don't have anything here at the moment so we're just going to populate these feature groups to get started and basically what a feature group is is think of it as a panda's data frame it could be a spark data frame um it can be even for Flink it can be a data set in Flink but basically we're going to write it to the platform so I have a name for it I give a description a version um you can define a primary key on it which is a good idea if you've got unique rows so in this case for um when I'm inserting uh users data each user has a unique user ID so I say that's the primary key I have this thing called online enabled and I I'll go through that in a second but one thing makes a feature store different to like let's say Dropbox or Google Drive is that I'm not just storing the panda data frame I'm actually going to put it in a table in a low latency database so I can retrieve rows of it at very low latency and then finally we're going to pass into this expectation Suite because we want to do the data validation in here okay so you can see here it's now um uploading my data frame and uh that'll get started here I'm can to do the same for my videos and interactions we start that as well um so it'll take a few seconds given the internet here let's see how long it takes didn't seem to be too bad um that's 25,000 rows this one is 25,000 rows and then this one is 250,000 rows so you can see it's uploading yeah it was quick enough actually uh but what what it's actually doing is it's uploading it to the platform and then in ingestion job will get started to to ingest that into the two different databases the low latency database and then the uh ultimately it's storing it in in what's called a table format so you've got aache hoodie and Delta Lake and hopss it's going to be a table in one of those as well so this one's a bit bigger it's 256 250,000 rows luckily this is not internet here in Germany actually I forgot this is running on collab that's why it's faster um and it's uploading the data relatively quickly there okay so um that's the start right so we've created some base data we're going to use this data then to create what we call a ranking model we're going to look at retrieval that's just us getting started so you can try and get started my co colleague here Javier will be moving around if anyone want to raise up their hand and it's stuck he can try and unstick you for a bit uh we have one up here already any questions it's a big crowd but don't be shy do get oh canot do we have one but I can't see sorry to what the go is there places see the Google collab link let me show you how to do the Google collab link I hope everybody has this link at the beginning right okay if you haven't got this link at the beginning you want to get that link and open it now I'll leave it here for 10 more seconds shorturl.at but then you have to go to the slide 27 there is it says instruction and another okay has everyone is there anybody who hasn't gotten this link yet I I'm going to take it off the screen with the okay so let me show you how to do collab because we need to get into collab here so when you go to slide this is this is Tik Tok tutorial instructions you can see there's um there's the the link to the Google Doc here if we open up the Google Doc here you'll be able to see when I zoom in h you'll be able to see this T Tik Tok tutorial here now I jumped over some steps right I'll admit that I jumped over over quite a few steps um okay but let's let's go through those steps those of you who don't know collab you need to go to cab. research. google.com and cab. google. research.com if you don't have a Google account I'm sorry I can't help you uh you'll have to live with the the Wi-Fi here um but you can see when you go to google. cab.com you've got some options on the left here so one of them here is GitHub if I click on GitHub and I paste in the URL that we had earlier and press return turn what we'll see here is it'll show me there's a bunch of files in there right authorized with GitHub yeah whatever okay this worked earlier I promise oh look something it appeared down there so maybe we can just press cancel no okay anyone else get this obviously too many of us are doing at the same time sorry no this is right no no it's the wrong one you're right it's the wrong one thank you thank you thank you thank you you got me okay so um thank you we have people paying attention not me so that one I showed you actually is uh is is the more complex one Pi data uh Tik Tok is the correct thank you very much you got me all right so let's put this in here is there typo there there again ti ti TIY talk Ticky talk Ticky talk that sounds I prefer Ticky talk it sounds a bit kind of sounds more fun less less than self harm videos isn't it Ticky talkie check that URL if that domain's not gone gone I'll buy it before the end okay um sorry so that's a mistake thank you very much for picking it up um so this is the repo here Pi data- Tik Tock and um when we go back to collab um not this URL here we're going to go to hi data- Tik Tock the other one the other URL if you're curious has all the Flink code and all the kind of fancy stuff I wouldn't I wouldn't touch it you huh yes you're right I I need to put it in here don't we yes there we go okay so then then when you go to this one here this repo here we get the link here um feature backfill right now the first thing you need to do here is you need to uncomment all of this right and the way I do it here's some tips uh Pro tips here control a go on control a yes and then control forward slash and then it uncomment it all right I'm sure 99% of you know those shortcuts but anyway I just want to sound like I knew what I was doing um and then you press return H shift return and then it'll it'll uh install we need two libraries for this notebook one's hopss and the other one's memesis God knows why we need memesis I really don't know um I would love to say I wrote all the code but I didn't uh Max wrote most of the code on this and um and that's basically it so anyone who's stuck Javier can kind of help you get around that any questions and any of that just regarding the next steps you B something from yeah so the other one that you want to do is you want to create an account on hops works right so we're going to use hops as the storage layer it's going to store our data and it's going to store um so you need to create an account here so if you have a Google account you just press Google continue with Google press press and it'll create the account for you yeah I'm not going to I don't I if I press this it'll just log me in right because I have I hav't I'm not registering account it doesn't it literally asks you for two things when you're registering I think your name and what you do or something like that so and then I need to create a new project h no it'll create hopss will create a project for you so it takes it takes about a minute to create the project I think name oh yeah you can put in the name then yeah yeah you do need to sorry I didn't I haven't registered you used to create a a project for you automatically the namespace for projects is a global namespace so if you call it Pi data I bet you someone has taken it so be original and call it Pi data 24 um and I bet you someone's taken that one as well Ticky tuck that's a nice name Ticky tuck don't anyone steal that on name cheap I'm going to register it after this all right um any other questions all right so let's have a look what happened right I think I did I run all these I didn't run all of them let's run all of them so that we kind of get to where we want to go uh I was here okay yeah I'd gotten this far so I what I'd done so far here was i' i' I'd uploaded those three data frames Panda's data frames and when I press refresh over here my feature groups it didn't refresh automatically but I get those in here you can see I've got the users the videos and the interactions so you can go in and have a look at them maybe you can press data preview no data there yes okay U that data should be in yes so it just appeared there one second let's go back here if I click on offline well we can see some of the data is in here so I click on offline here and then the data went in here okay so you can see some of the data in here it also computed statistics on the data that it WR wrote in excuse me you can see the age of the users their Min their Max the mean some the country there's 239 that's more countries than there are in the world that's unusual and there's three genders and so on and then the same is uh the case for videos you can see there's some videos we can click also down here on activity to see what happened and you can see it wrote 25,000 new rows and then if you go back here to um interactions and we look at activity again which should be there should be 250,000 rows at row okay so that looks okay um right so what I'm going to do next so so now those three tables are in there I'm going to create something called a ranking feature group and I haven't explained what ranking is but I'll just go through the code just to get this notebook done but ranking um ranking is what it's going to do it's going to personalize this data to me I I want my recommender to know and learn about me so if we look at at at the columns that we have in our tables before we we do any more let's look at the columns we have a user has a user ID a gender an age a country not a huge amount in there of course you could have a lot more data in a real system the videos have a category a number of views on them a number of likes on them a video length and an update length now this might be updated once a day you might have a big batch job to update the videos or maybe every hour depending on how how frequent you do it um users might get updated you know um once an hour as well or less and then we have these interactions and these are updated the whole time because users are clicking on things and and making updates so we can see the interaction types so we're going to have a number of interaction types like you watch a video or you like it and then how long you watch it for so we have a a user uh this user watched this video um for this amount of time and then each interaction will have a unique ID as well so it's going to be a primary key identifying this interaction as well so given that data and I want to train something call a ranking model I'm not going to get into the details of of what ranking models are but what they are what they basically are trying to do is personalize recommendations for me I'm going to get back you know anywhere between two and 500 videos that it could recommend to me I want to personalize them to me so in order to personalize them to me um we need to create a data set that I can train a ranking model On and Here what we can see is in this data set I have a user ID a video ID um that means this user uh watched this video which was Sports and that that video had 39,000 views and 37,000 likes and it was 25 minutes long and it was update uploaded this date and and then the gender of the person watching was this and the country and the label and so on so then we have this thing called label at the end see this label any guess what that label is we'll see it here in the comment here here it is here it's defined here anyone to shout at me what they think it is the W of the type Direction kind of anyone want to sh what they think it is it's positive or negative right so you can see here it says that if the user viewed liked shared or commented on this video that's an interaction right when I'm building a ranking model I can't just have ranking data where users interacted with them I want negative ones as well where the user see we got a label zero here right I saying this this user didn't watch or like or comment or click on this video we need to have a mix right so here you can see we've got some data where we have in positive interactions and somewhere even negative interactions what you can say see from these features here these are all the features we can find out that oh if I'm a user and you know my age is this and I come from this country other people have like tended to like this and other people have tended not to like this you can see that that that signal is in the data right we can see that's there yes now you can imagine personalizing it even more and then storing features about exactly what I did right what did I do the last hour what it do last five minutes last 20 minutes this is more simplified version of it excuse me okay so I have another feature group for that um um online FS um okay any more questions on that so I I I've run all that notebook that's my feature pipeline no I have have I run it all did we we'll close that one there okay uh here's my notebook am I finished it was I I'm finished all right it gave me what is it saying ranking DF not defined I missed the cell sorry let's go back I didn't run this hell did I all right so going super fast here I don't know what's going on is thinking about it we're killing the internet right let's go back and do a bit of theory while that's spinning around there I don't have the patience to watch things spin I don't have much patience at all right keep an eye on the time we're good for time don't worry okay so we said that we're going to build the AI system out of three pipelines feature pipeline training pipeline inference pipeline and the feature pipeline so what we did there and I'll explain kind of what what what that code was doing is what we were doing was simulating um creating the training data for something called a two toer model I'll get into what the two toer model is in a minute but to build a two model we need to store the interactions between a user clicks on something or user views something or comments on something you need to log that data so if you want to build um you know a personalized recommendation system for your ret site today you're going to have to start storing that data it's the first thing to do and you can have a like a a a label where you say oh the user clicked on it one or zero um you can have you can see I've got scores there 0 one five you can have like um a score five if you put something in your shopping cart one if you clicked on it you know uh if you share it give it a two you can basically come up with different scoring mechanisms to basically um help your recommender system over time time but we need to log that data without that data we'll see that it won't work and um I'll explain in in a second why that is but these were the tables that we created here right um we have the interactions here on the left and these were our um we had the you the interaction type the watch time and then we had references to the user IDs and video IDs this is something called an entity relationship diagram in case you're curious in in database world and then we have we have a users table we call them feature group and we have the gender Age Country and then we have the videos where we the category views likes and upload and so on what we haven't looked at yet now is something called video embeddings so we're going to do is we're going to create an embedding a vector embedding from for each video so remember what a vector embedding is it's we take all the information we have about a video and we compress it into an array of floating Point numbers a fixed length array of flowing Point numbers so it keeps all of the semantic information about the video I'm able to say is this video which videos are similar to this one and I give it the array and it gives me back arrays for videos that are similar and that's the basics of um of vector embeddings and similarity search but we're going to do that um in a second and we can see that you know we have a batch Pro pipeline typically which would up um update these now in the streamlet example later on that we look at the UI when I actually click on the videos and and and like them it'll push it into CF and this gets pushed in here that's how interactions are getting in there in a a a Tik Tok style system but the other ones are updated by a batch pipeline that batch means running on a schedule like once an hour once a day once a week streaming means it's running all the time 24/7 days just keep coming in so I said I I'd tell you what a two- ter embedding model is um and it's actually something that's quite difficult to understand uh I it took me quite a while to get my head around it uh and I'm I was a professor like you know so that just tells you nothing but um the it doesn't mean anything like but anyway um everyone knows I think everyone probably everyone in this room has seen the basis of what we call similarity search you go to uh zelando and you look at uh some shoes and it shows you shoes that look like them right and many of you will know that you can take the image of the shoe you can convert it into this array of floating Point numbers fixed length call the vector embedding and then you can go to something called a vector index or ctor database and say show me show me similar um pictures to this and you'll get back shoes that are similar right similar color similar style or whatever um you can do that with lots of things not just with images you can do it with text you can do it with um you know tables of data about users and so on but what's really hard to do is what we're trying to do back here right which is saying a user clicked on a video and the user pre prly had looked at these other videos and the user likes these other types of videos and the user is this old and the user has this age we have information about the user and then we have the information about the videos and they're two different worlds right they're two different tables yes the user tables here it's completely different data from the video data but I need the two to interact because I what I want what personalized recommendation means is that I can take the user data and recommend a video right but if I generate embeddings on the video data and find similar videos the user is over here it's not touching the users so how the hell do I get the user data and the video data together so that I can say this is the user recommend a video that's hard right they're two different two different modalities one is videos one is user queries and user history so the answer to that is something called the two to model what we want to do is if we take a a a video and we're going to and we have the road that describes our video and we convert it into a vector embedding and put it into our Vector index we'd like to take the user queries and their interactions when they clicked on a video when they liked the video when they content a video and put into the same embedding space the same Vector database the same index the same length of the vector embedding this is the basis of uh the two ter model let's look at an example just going to show you an example of of it working and you'll hear the term uh not video but it's a it's a book we got a book called Rome and jul Juliet it's going into the embedding space Great Expectations King L these are all famous books in English pride and prudes now we've got an embedding space with books in it user queries come in oh look Shakespeare try is close to Romeo and Juliet and Kingler how the hell did that happen right like cuz one was a user query and the other was a book how the hell did they get close to those well we had interactions when a user queried for Shakespeare tragedies they tended to pick those those books now we've got an interaction we've got we've got some training data which says that this was the user query this was the book they clicked on or the book they liked or the book they commented on now I got some training data that I can actually put both of these modalities into into one bigger modality is that okay from a hand wavy perspective yes handwavy feels kind of yeah kind of get where you get but not really um let's look at it from a little bit more technical perspective right on the left we've we've collected all of this training data the users clicking on videos right what we're going to do is um I guess many people have looked at llms and and Vector embeddings right you've tried out sentence Transformers or something like that and you've kind of run some text through it and it generated a vector embedding went oh that was great what you did was you used a model you used a a an encoder model right and that encoder model was trained by someone right but what we're going to do here is we're going to actually train our own encoder models we're not going to take an off the shelf one from hugging face we're going to actually uh train a user query encoder and we're going to train a video encoder and the way it's going to work is that we're going to train them using this interaction data so when the user you know clicked on the video we'll get uh uh this highest interaction and when they didn't click on the video we'll get a lowest interaction of zero and what we basically say is that when a user entered this query or clicked on this as their most recent video sorry or or this was their history and then we present a video and we get a zero which means that there was no interaction there that means that we have a a that the that that the we had a loss right in the model that was not close together close together in this case means often dotproduct or cosine similarity we look at the at the the angles between these two different um vectors and then we basically say if the loss is high because that's what we got here we've no interaction we're going to update the weights on our encoder models using gradient descent and backrop and um if the if however the others they were close together if the users's history and so on was was uh match to a like for this video well then we're going to get a a a one out which means that that we have a lower loss which means that we're going to update the weights to say that was a good thing they're closer together so what will happen is the embeddings over time will tend to uh collocate in in high dimensional embedding space so when users issue queries or whatever in this case clicked on videos related to sports um and then we have a video that's in the category sports they'll tend to be closer together in this joint embedding space we have a question y does each embedding represents each user and his interaction with video so the we have a ve so I'll get that in a second so so we have these are are called Vector embedding models what they do is you put in some input features and you get out a an array of fing Point numbers yes that's what these two things are the models but the actual data this is what we do in the next slide so it was a good one for the next slide so what we do is we take all the videos in in Tik Tok and with this Vector embedding model that we've trained we compute vector embeddings forther and we write them into the vector index yes okay so let's do that let's do it now and I'm going to pull down my glasses wherever the hell they were because I'm an old guy it'll happen to all of you too at some point um let me run up my laptop because I'm honestly tired of collab uh let me go here right so um so what we're doing here then is um we're going to use a package you can see it's complet exp in here I pip installed tensorflow already it's basically saying I've only got um I've got some avx2 instruction set I don't have a GPU in this machine it's just a warning don't mind us and but I'm using two two libraries here one is tensor flow and many of you look know tensor and say well what the hell are you doing everyone's using pie torch tensor flow sucks which is true but like tensorflow has still this tensorflow recommenders Library which is fantastic for doing what we want to do here right we don't have to write much code to do a two to embedding model and tensorflow recommenders so we're using it because it's the easiest way to get it done you can go ahead and write everything from scratch yourself and pytorch uh but I'm not going to do it so what I'm going to do here is I'm going to basically um first we connect to Hops works we get the references to this data we had before the users videos interactions that we we downloaded and now what we're going to do is um you can see we're going to create this thing called a retrieval data set now I haven't introduced the term retrieval but often the the two Tower embedding model um this is what we in in in recommender systems we call this a retrieval phase what I'm doing is I'm putting all the videos in my Vector index I'm going to take the user query compute an embedding find similar videos and then get back those down as my first phase I'm going to get like a couple of hundred candidate videos back so we call this the retrieval phase in in um recommender systems now to build this I need I need the retrieval model so we're going to get some features so we have the user uh gender Age Country you're probably breaking the law here using gender you shouldn't train models on gender uh I'm doing it anyway and um now so oh I'm going to be stuck in the internet here so this will be interesting um so what I do is I create a feature view now in hopss what a feature view is is basically I want to take features from these three different tables so I've got the um users the videos and then the interactions between them and I'm going to select the interaction ID from the interactions um uh feature group going to join in these features here the gender age and Country joining them on the user ID and then they going to bring in all the videos features the category the views likes and video length joining on the video ID here so um with those selection of features I create this thing called a feature View and then if we open up hoper we'll be able to see our feature view here so the feature view will appear here this is my retrieval feature view it's just metadata right it's basically saying oh you know I've got some feature from the interactions feature group some from the users some from the video it's not supervised learning so I don't have a label or a Target column you could Define a Target column as well um and what I did here then was I can create uh training data from the feature view so I can get it back as a as a panda's data frame now what I did here was I called train validation test spit so I can actually um break up my data into um training validation and test sets and I can specify the size of those training and test sets um and then I can go and train a model so um this is where this is where we're going to use the the two Tower embedding model so this code will not be familiar to many of you if you haven't done this before so don't feel too kind of intimidated um but we have remember we have thing called the query embedding and then we have the video embedding or candid at embedding so there're the two inputs that we're going to have you can see this one the query one if we go back to our slides we can see it there this are the the query coming in here that's those features coming in here and then we have the video features coming in here all right so there're the the features that we're going to come in tensorflow doesn't like Panda data frames so we have to um convert the panda data frame into a data set a tenso data set and that's what we're doing here we're doing it batchwise so I should run that code um I don't have a GPU so it complains a bit and um yeah so basically then I'm I'm I'm just making sure there's no just counting the number unique rows and items now this is where we get into the two toar embedding model um this is a very small uh embedding size so 16 a length of 16 and the number of Dimensions my embeding model it's not very big but it's okay for the moment and I'm going to start with the query Terror remember we have we have two parts we have the query Terror and then we have the the video Terror or item Terror I'm just going to run both of those while I'm here sorry um I'll start the training as well just because it'll go off okay so we started with the user Tower so let me go through the user Tower here um we call that the query Tower as well so we have um it's a it's a class it's inheriting from carth's model so we have an init method and in the init method we're going to set up all of the features inside this user query so we have firstly the length of the the vector embedding which is 16 16 floting point numbers um we can see this is our embedding model itself the user embedding and what we're saying when we we create an embedding model we have to tell it what are the users the list of users and we we calculated that earlier in the previous cell we got our list of users here on our list of videos so we pass that in as the vocabulary to embedding model and then we tell about the embedding model we say hey um we're going to have um the embedding will have this many uh uh tokens in us and um yeah so that's basically we this one embedding layer so it's not a very complex embedding model that's okay and the output will be 16 flowing Point numbers then we have other features we have the age of the person we're going to normalize that we have the gender we're going to um uh I think we're one hot encoding it but we're we're basically tokenizing it and they're doing the same for the other categorical feature countries and then we basically Define a feed forward Network and the feed forward Network here um takes in our embedding and we have a reu and and that's basically it that's the the definition of our embedding uh uh model the layers when we make a call on it we want to train on it or or make predictions we'll go in here we'll we'll get our um our gender will be one hot and coded our country will be one hot and coded um we'll take our our user input data and we'll um concatenate that together and then we'll run it through our feed forward Network and get the output put and return that's basically what the uh the the query embedding model will do when it runs so we Define that one there and then we create an instance of it and um we're also passing in the the normalizing the age as a parameter we're doing that there and then we're basically figuring out what is the data set that we're going to train this model on so that's this query DS and then we have our query model is passed against that query data set that we we we run that on we do the same for the videos this the item to we've got again the embedding length the same um embedding model that we defined earlier for users we're using for videos and the same tokenizers we're normalizing the numerical features um we've a a feed forward uh model here again with the r activation unit and the call is the same as we saw before so both of these are very similar the item Tower and the user Tower um and we basically um um we initialize them here so both of them are now initialized but we haven't done much on them we're going to do our tring with this thing called the two toer model so if we think about it um at a high level we go back here we' basically defined that we've defined this but we need to kind of put them together so that the training data can go back here and then update the weights in both of these in parallel all right so that's this is our two ter model so what it has inside it is the query model as one and it has the item model or the videos and um then we have our data set in here you can see here um so when we do training TR in you can see it basically um computes the uh user embeddings the item embeddings uh we compute the loss and um and then that's basically it and I think I've ran this already um did I run this already no maybe not okay I should have run it already Okay so so basically we're going to we instantiate so I didn't write this two term model we we're using the the the tensorflow so we look earlier um we're inheriting from tensorflow's Caris U model interface and then we can use some of the liaries that they have as well to uh to make this go better so actually training it doesn't take too long you can see it went pretty quick um but once we uh trained our our our models we now upload them and we're going to store them in hopss okay so let me tell you what was happening there so um in this case what we did was we had our query model we have the two models come out the the query model and the video model so um what we need to do in hoper and this code is somewhat simpler in the latest version um but we need to define the scheme or input to the model and um here we can see that um the output is the the actual Vector embedding um but the input is the same interface as our as our data frame our query data frame so the query data frame M had the user's age and then their uh country and so on um we need to do the same for the video one and again we'll take the the input schema for the video from the data frame and that's done down here so the that's done down here in our is our candidate or or videos model we again take the uh data frame from that to create the input scheme and the output scheme is the same so then what we need to do is register the model once we defined its schema um we've also added a sample of Records so we can store them in in hops works and you can try them out then but here we're creating a tensorflow model so this is basically the syntax we creating a tensorflow model and then we save it here we say save that cre model um and that c model uh is uh was saved to dis I guess up here yeah so earlier on we saved the C model to dis on this line and this line you can see those two lines so after we run these Co these lines the the both of our models have been saved to local dis to a directory uh called ccore model and then item model and then when we call in hops works you can see it's doing that uploading thing there it's uh this is basically uploading the contents of that directory to hopss now we need to do the same for the video model I'm not sure if I run this already um it uploads it and then we can go to hopster to see our model should be up there um which is down here okay we've got our models are up there now so that's our training pipeline any questions on that that's for the that's for the ranking for the for the retrieval model we still once we've created that we need to now populate the vector index we need to take all of our videos and use that um what we called the the the item uh encoder or video encoder we're going to take all the videos read them up run them through this video embedding model and then insert them all into a feature group in hops works so that's the next notebook which is here Vector embeddings creation um you can raise your hand as well Javier can go around and look yeah go for it I'm not sure IOD this correctly does the video EMB information the content of the video so we can look at the columns we can see what the embedding does so the question is what does the video embedding contain well this is our video here you can see every row we have in our video here if we have a look at a row here let's have a look at some rows they're slow let's have a look at the overview of the row this one's a bit slow um so we've got we've got a category of the video we have the number of views of the video number of likes of the video the length of the video the data was uploaded on okay how do you you handle thee of yeah so this is what we call a batch feature pipeline right we would need to run it on a schedule so um you could run it every 5 minutes if your Tik Tok run it every minute or something you know so when you upload your video to Tik Tok it's put into a big directory and some big program will take that video in its description and it will write and update this feature group yes and then um a video embedding uh this what we what I'm calling here a vector index pipeline will take all the new videos that have been uploaded in the last 5 minutes or whatever and then compute embeddings for them and then write them to the vector database in our case a vector index no user interaction there's no user intera no this is unrelated to user interactions but it was trained remember using user interactions we needed to have the user interactions to train this video embedding model and to train the user embedding model excuse me so what I did here was it created a candidate embeddings feature group and if we go back to here and we reload here we now have a candid embeddings feature group and this feature group contains our embeddings all right so we just have the video ID and then we have the um the array of of 32-bit floating Point numbers right so 16 floating bit numbers so now like with this feature group you can call find similar you can pass in an embedding and you can say give me similar videos and it'll give you back the the IDS of the videos that are similar right so that's how we do similarity search now we don't have a separate Vector database for this this is done in hops you just treat as a feature group that's why I use the term Vector index now instead of vector database um those of you who know the likes of postgress you know postgress has its own Vector index Neo J mongodb every day so it's an open question as to whether Vector databases will be an independent category in in a few years time because so many databases are adding support for Vector indexes today um okay a question your question online yes how you evaluate the quality of embedding models how do you evaluate the quality of embedding models I didn't do I didn't show you anything there did I so normally an embedding model is an encoder right so what what we're doing here is we're we're we're we have the training data is the interaction data and we're basically putting in the data and the model predicts whether it's a zero or one an interaction or not an interaction so what you do is you you in this case we're looking at the distance metric we're looking at the distance between the predictions and then the outputs so you're basically trying to minimize that loss over time so that's all I'm doing in this particular case is I'm trying to minimize that loss in training now whether that's a really great uh you know so this is not supervised machine learning so um we're not going to use any of our classical like supervised machine learning um evaluation metrics for it uh we're just looking at the distance between the predicted uh interactions and whether it was correct or not all right so that's that's number three finished so let's get back we're we're starting to lose a bit of energy here I can feel it come on we only got 20 minutes left let's let's try and we're at we're at notebook what where are we at number three number three of five okay not too bad so that's what we did here right you might think like Jesus God God he would he said feature training and inference on your training we did training already but we're going to do the second training uh we're going to train this ranking model which is personalized to you we actually created this feature group for this earlier uh one thing I wanted to just say to you those of you who who kind of work in the space if you have time series data so time series data is this is all time series data in the real world right uh users have a a column saying when they were last updated they have video when it was uploaded you know they have uh uh interaction the time of the interaction creating that training data is quite hard right because you know your age Chang changes right so if I take the interaction from two years ago and I had a younger age than my not so young age now and um you know if I had if I had the wrong age in my in my training data because I just pulled the latest value of your age from the users table I didn't know what your age was two years ago I don't care or whatever just pull whatever's in the user table and join it with the interactions then I have i' have stale values right so i' have some stale feature values in there it wouldn't wouldn't make my model work that well there's a worst case which is where you have what we call Future data leakage so you've got some time series data and you're trying to take the the interactions as your starting point there there's going to have we're going to create train data from there we're going to pull in some features in the users pull in some features in the videos you know but if you predicted things that you knew were going to happen in the future you got what's called future day leakage it sucks right because then your model your training and testing goes great but then in production it doesn't work at all so here's a little tip how to create training data from time series data use an as of left joint anyone know what that is right it's not so easy right um so that's where uh feature stores will help a lot is that this is something called slowly changing dimensions in the data warehousing world it's a very complex topic but in our case it's pretty straightforward um we don't need to care about something change of Dimensions we don't need to care about Les as of left joins uh what we're just going to do is let's run through the notebook here we're basically going to create some training data from our feature View and the feature view in this case I'm going to create a couple of other feature views just because I'm going to read from videos and users and but in this case I'm going to use this uh ranking feature group and the ranking feature group pulls in features from the users and the videos tables but it does it it runs this as of um uh left join and the background to make sure they all get pulled together correctly okay so I'm just going to train this ranking model while we're talking here now this ranking model needs to be super fast right um the reason why it needs to be super fast is because we need a very low uh endtoend latency and um gradient boost decision tree models are really fast like they'll make predictions in a millisecond or two or three um and cap boost is is one variant on it it's categorical uh gradient boosted decision tree with typically mostly categorical data as input um but basically I trained a C A C boost model there and um we can test see what its performance is um you can look at the the loss here so we can't see it the loss goes I think it started about 50 60 and then it drops to about 42 so it's not like this is not the world's best model it's not the point um but we've training a Model A C boo model um we basically just do some simple uh ADV this is some because remember we're not talking about deep learning models anymore we're talking about decision trees so we can do things like feature importance we can say Hey you know I have these features age likes views country category video length which are the most important well we can see here the age is the the the the largest indicator in this particular training data set um and then we can basically add that model again to hopss to our uh yeah to our feature uh to our hops platform so we're going to upload the model and um let's have a look it should be here somewhere okay so now now we've got our our model up here as well right um it's called the the ranking model you can see it here and you can pull them open and you can see we can also store you know when we upload a model we can have Precision metrics you can also store images PNG files so I know a lot of people use um you know weights and biases and um you know Neptune and a bunch of other platforms and mlflow they're all really good but I think like here when you save a model if you store you know all the feature importance images and maybe even training loss in your model registry it kind of is a poor man's uh experiment tracker I I it works enough for me because I don't store experiments for models I don't save put it that way all right so now we have a a ranking model okay we're making progress so that's basically what we kind of we we did like at at a high level what we talk about um in machine learning are what we call assets so like you know I had these views of all the features I created some training data sets the retrieval and ranking training data sets with them I trained the video embedding model the user embedding model and the ranking model and I put them all in the model registry they're the assets I have now I've got the feature groups I've got my feature views I've got my models um when I put those steps together what we've done already what we basically do you can think of this as being the offline things that happen this is not the online system the offline things offline things are are things are happening in the background so batch pipelines streaming pipelines our data is arriving in here um we're doing some feature engineering we're storing it in the feature store from there we have the model training runs and it can run at its own Cadence run once a day once a week once a month um we have the insert into our Vector index where we just said how often should it happen if it's you know Tik Tock it should happen very quickly right within a few minutes maybe for you you do it every hour or every day but that's a program that needs to run on a schedule take the new videos that are added compute the embeddings for them and write them to the vector index and then the ranking model how frequently should we retrain that well you can do monitoring you know hopss has feature monitoring built in you can look for um you know if your model predictions are going stale over time you can basically see drift in features and so on then you can decide to automate retraining but often it's just the case of just running it like once a week or running it when you see um the predictions getting worse so um there's one last part to do um and then we're kind of done in training and it's what we call the deployment pipeline right and that's creating going from a model um as just a you know uh in this case an asset so it's just a file like so my my cat boo model is a pickle a pickle file we can actually see them if we have a look at them let's go back and look this is our ranking file here you can see we're up here it says browse model files well we have the Pickled python file it's 426 kilobytes it's not so big we have a Json for the schema you have a quick look at that um the other models that we had um the the let's have a look at them here the uh the query model and the ranking model and you can have many different versions of them of course um you can see here that um this one is slightly different so in tensorflow models orpb or protocol buffer files and then we have a bunch of um weights in here in this folder the variables folder so they look slightly different okay so that's all our models are but when we want to use our models we want to make them accessible from somewhere right so there's two kind of this is what I often people often call a machine Learning System right so a machine learning system or AI system is the system this is the operational thing the thing that runs you know takes the um user input uses the model makes predictions and goes on and but that machine Learning System could be it could be just a python app with a UI like streamlet that downloads a model and makes predictions you can do it all in Python that's one type of machine Learning System another one is where we host the models on a network endpoint and make queries on them and that's what we call a hosted model so it's hosted and model serving infrastructure that's what I'm going to do here you can even have another case which is where you have a stream processing application that has an embedded local model and as data keeps coming in it bits out predictions that's another class of kind of machine Learning System so in this case I'm just going to create some deployments and we'll uh excuse me um we're going to deploy the deploy the rank uh the ranking model first um I'll explain how this works so we're using something called kerve kerve is kind of the de facto open source framework for model serving and um what I'm doing is I'm firstly I'm finding the best models you because the models have a we we put the the performance of the models in there and I say get the fscore that's Max that's my best model get that one and then um in kerve we have two components I'll show it in slides to make a little bit clearer um let's have a look we kind of skip over this this is kind of what what what we're building is retrieval and ranking we we retrieve videos uh from our Vector index and then we filter them out and then we're going to rank them with a ranking model but this is what we're kind of and I'll skip over this is just the same thing again this is really what it this is the nuts and bolts of it our user interface app will go to something called a Transformer this is a Python program that we wrote and that Transformer will call the user cre model so the user um recent history will come in here we'll go to the user query model to get the the vector embedding for it then we'll go to the next model the ranking model um and we'll say hey here's the user query uh the embedding from the from the user query that we've computed it will go to the vector index find the closest videos then it will go to the ranking model and say please order these videos for me and then it will return the the ordered videos for you so it's using this Vector index and feature store to basically first the vector index is to find similar videos and then when it's found similar videos it goes back to the feature store and says give me all the features for these videos because the vector index didn't have the features it just had the IDS for them so it goes back and says hey I got like 100 videos give me all of the um the features for them so that I can make rank them for me so what we need to have then is these programs a Transformer program a predictor program a Transformer program and a predictor program this is the way K serve Works your request comes in goes to Transformer first then it goes to predictor in this case we have two models so we go from the Transformer to the predictor and then the Transformer will call the second model this one the predictor model and then that will come back and answer so that's what's happening in the code here and we basically Define this uh Transformer so you can see it's doing it slightly backwards it's h got the ranking one first so so inside here we basically connect when we initialize our Transformer we connect to hopss we get a reference to our feature view we get the list of features um we get a reference to the to the embeddings as well and we download our ranking model and then when the request comes in so this is just the initialization step when the request comes in what we do is the request looks like this it has a number of instances these are videos that we're going to rank um and the first thing it does is it finds the closest videos so it says hey I got in this user input here it's going to be the the user uh the user embedding and says show me the 100 closest videos to it okay it gets back to hundreds closes videos it removes then we do a bit of filtering here we remove all of the features that uh the user has looked at before right now you could you should probably do that saying the last hour or last day often you'll do filtering like hey is this user under 18 remove all the videos where the the the movie is not for under 18s um then we get a a list of these videos that are filtered we put them into a data frame and then what we do is we call ranking on the data frame so we're basically going to say okay this is my ranking model inputs this data frame I added the the video features to the to the videos that I that I got back we've added a couple more features to them and then we call the ranking model which is um basically going to make the prediction so that's going to be the predict program so at this point this return actually goes down I go down here it goes into um this predict method here and then what it does in the predict method is it pops out the features and videos and it calls model. predict so this is our our um the uh cat boost model that's that's ranking and ordering these features and Returns the scores and the video IDs and they return back to the postprocess step here they return back in here and then what we do is um we sort them like we want to sort them by their scores and then return them ranked so that's the two programs the Transformer and the predictor let me uh run these briefly here and then we do the same for and we and now we're actually going to start that model running as well so we're deploying that model and we also have the um yeah further down we have let me go down further to the uh the second one which is we'll go down here okay this is a test for us let's skip that first we'll just run the other one the query model so we need to deploy the query model and uh let's do that here okay so let's go back and look in hops works and see if they've started there so yeah we can see now our ranking model has been deployed and we can run start on it up here I don't have a huge amount of time left so we're going to kind of move through this relatively quickly um okay um it's starting so hasn't quite started yet and then we can basically do there's some cells here that will test out your recommendations now the one thing you will need to do here is you can see it has this user ID here so you actually need to change that it won't work in your code because I just picked a user ID out of my uh out of here so you need to basically go back here to your users and then click in data preview uh reload this page what happened there um and then we can take any of these user IDs here that we can see so I can take maybe this user ID here or this one here and paste that into my code because otherwise it'll try it's going to look up a user ID that doesn't exist right so it needs to be us ID that exists and then there's a query embedding and so off we go okay so look looks like it started you can get the logs there tried out if you want excuse me and boom there we go and then our second one here should have started as well at this point yeah it started okay all right we're good to go okay so that that's it it's kind of done right so then now the these cells are just to test it to see if thing is working we can basically go back and look at our actual system so this is our system here now this system here is a is a streamlet program and it's using our model let's look at the code before we move on um the code is down here it's in the it's in the streamlit um folder it's called app.py so what this program does this is our inference program our online inference program it basically gets a reference to our our deployment we get a reference to the feature views um it prints out some stuff Employments are are um are ready when I click on if I click go back here and click on one of my videos here okay I've clicked on one and it's doing a recommendation what it's going to do as well is it's going to write that directly back into the platform so that was that code that I looked at here so it's getting my recommendations now not super fast um but it's basically inserting that back in so that every as soon as I've clicked on it it's writing it back in so that it knows then for the next prediction um whether it's recommended or not okay so it brought me back some recommendations here so then I can click on the next one and then it'll recommend the next one it's taking a couple of seconds but um not to worry so that's this is the whole user interface is written in Python in a framework called streamless um it's not that big and not too uh intimidating I think okay so there's it gives us some recommendations there so that's basically it um I'm going to finish up wrap up here and um say look you know there I understand there's quite a lot to go through in this this is not something you might do in in 90 minutes um but it's doable right it's like what do we have six python programs not super hard um this thing that I'm showing you here is a streamlet app that's running I'm not paying for this um hoper is free forever on the free tier so you don't have to pay for that either so I can go off and show people hey look I write a Tik Tok recommender system and go for a job interview I don't need a job by the way in case you wondering but um I could go for a job interview and say hey look I built this and explain it to you this is pretty cool right this is like a Step Beyond just training a model on some static data and making some predictions right so if you're interested in that General concept um you I'm writing a book on O'Reilly there's a few chapters out already but you can download the first chapter for free there's also a community called serverless ml that I'm involved in where people build these serverless machine Learning Systems um and you get some good tips and hints for how to build these systems so that chapter is free and it'll kind of explain a lot of the what what we went through today and obviously llms will come in at some point not in the first chapter but uh after that that's it I'm out of time thanks for listening and uh putting up for me and uh TI dat is just getting started so have a great rest of the conference um maybe see some of you around um I'm with hops works we have a booth upstairs if any wants to chat to us you'll find us up there there's 5 minutes for Q&A does anyone ask a question she didn't tell me that before sorry so are there any questions no we're good let's go let's go get a drink we have a question online all right go on sorry she's showing me signs I got two minutes left so I'm kind of you got two minutes left but we have so do you want me to read it yes just where's my glasses to put my glasses here Jesus Christ all right so I can okay so s says how can we also maintain fairness to different content creators and users exposed to new content that's a great question I don't have an answer for it um you know look my answer is open source right you know if that's your want answer second question is with the two ter model and similarity search you already have a ranking for users could the reranking be avoided having a single endtoend model that's a really good question and um yes you can of course uh avoid the the last reranking stage um any of you whove looked at llms will notice this sounds kind of familiar right this is like Rag and in fact recommender systems are rag but they're more sophisticated rag so if you want to do sophisticated rag do two- embedding model for rag don't just do Vector embeddings um the yes there is a point that the the first phase the retrieval phase is using your user queries is using your features and it's using the video data to find related um videos but it gives you a couple of hundred so there's many things you want to do when you're when you're actually giving back that I didn't go through you want diversity right you don't just want to give the top five or 10 they always ended up being the same you want to also put in some random selections for things like that that things that are trending right and then the reranking stage is really about the personalized the most recent things I've done and filtering and you kind of need that if you want to make it personal um otherwise it'll be kind of just generic it won't be as personalized to you that's it thank you everyone no come up after that I'll take questions after cheers
